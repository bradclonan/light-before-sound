# Light Before Sound - Research Overview

## Topic
Traversing through the vast expansion of neural networks in the human brain is an adventure pioneering scientists are on every day. With each new explanation a world of possibilities are open. One specific area of study that has helped us understand the way humans thrive, and survive is how we communicate. When communicating humans use many parts of the brain; for example try to say the word “cat” allowed. Completing such a simple task as speaking a word allowed is a testament to the complexity of the brain; first information must travel to the primary visual cortex, than the information is sent to the posterior speech area, after it takes a ride along the arcuate fasciculus to Broca’s area, finally making its way to the primary motor cortex, and out of your mouth all in a matter of milliseconds! Mapping the way we process sound and communicate have aided the healthcare field in finding new treatments for patients suffering for many different types of neurological disorders.

## Background 

Localization of function is a blanket term used to describe specific functions are served by specific areas of the brain (Fischl & Dale, 2006). Currently there are several areas of the brain known to aid humans in producing and comprehending visual or audio cues as language. Located in the frontal lobe in the left hemisphere is Broca’s area which serves as the motor area for producing language, lesions to this area produces a form of aphasia aptly named Broca’s Aphasia which can prevent a person from producing properly formed speech, though the person can still understand language. (Paul Broca, 1861). Wernicke’s area is located in the temporal lobe is involved in the comprehension of language, lesions in this area produces the loss of the ability to understand language; unlike Broca’s aphasia a sufferer can speak clearly but the words produced make no sense, and are jumbled (Carl Wernicke, 1879). Broca’s area and Wernicke’s area are connected together by the arcuate fasciculus a bundle of nerve fibers. Brain imaging techniques have identified another area that is connected to both the Broca’s area, and Wernicke’s area the parietal lobule. The parietal lobule lies at the junction of the auditory, visual, and somatosensory cortexes. In addition to being in this unique area the neurons in this lobule are multimodal (able to process different kinds of stimuli). The primary auditory cortex is used to receive spoken language, and the primary visual cortex is used to receive a word that is read. Viewing brain activation patterns of studies interested in determining the difference between how we use imagery “imagining a stimulus” and perception “viewing stimuli” of shown or spoken stimuli correlate to activation patterns in Broca’s and Wernicke’s areas. Furthermore, as with comprehending spoken, and written speech it was found that activity in the visual cortex in the occipital lobe resulted in the best prediction for what their subjects were perceiving. Activity in higher visual areas was the best predictor of what their subjects were imagining which in language processing the production area (Sue-Hynn Lee ET. All, 2012). When receiving either visual or audio stimuli of language we form mental images of these stimuli. It is easier to perceive audio presentations of words when in context than when taken out of context (Waldrop, 1988). Similar to audio it is easier to find letters when they are presented in a word than if they are scattered or in a non-word (Gerald Reicher 1969). Visual stimuli play a role in effecting our interpretation of an audio sentence (Michael tanenhaus and coworkers, 1995).

## Hypothesis

Currently the hierarchical manor of importance in processing language overlooks the visual aspect in many areas.

Processing language, sound, and visual stimuli together is apparent in all forms of communication. However, in auditory studies the visual aspect is often overlooked. Evidence from aphasic patients point towards visual (seen or perceived) being the primary “meaning maker” for processing both audio and visual language. Considering damage to the Broca’s area (motor control) prevents a person speaking language; yet allows them to still understand what is being said. Damage to Wernicke’s area (organizer) prevents a person from understanding language, and also prevents them from forming coherent sentences. A person who is blind can be taught to read with brail, a death person can be taught to communicate with sign language. All forms of communication require visual interpretations. Human’s process language and meaning in a hierarchical manor for both audio, and non-audio scenarios. Alternative hypothesis there is no difference in visual processing speed paired with audio.

## Brief Description of Experiment's

### Experiment Syllable match up

In this experiment I will be using a decision task which will consist of three different sessions. First being an audio presentation of a random word, followed by a screen to choose how many syllables were heard in the word. In the second visual words of varying lengths will flash on the screen, followed by a screen to choose how many syllables were in the flashed word. Condition three will consist of audio and visual stimuli presented at the same time, followed by a screen to choose how many syllables were in the word. The final condition will be an unpaired or paired matchup of a spoken and written word produced at the same time, first the participant will choose the spoken word, and second another screen will come up and choose the written word. Response times and accuracy will be measured in these tasks .The prediction is that the response time on visual processing will be faster and more accurate than the audio presentation. Also that when presented together it will take longer to respond due to visual, and audio being utilized. Furthermore the unmatched and matched pair test will help understand the accuracy of using both audio, and visual together.

### Methods

#### Participants

500 Rutgers University Students selected without pre-screening.

#### Stimulus

There are three types of stimuli used. For condition one the audio stimuli consists of a 1- 2second soundbite of a word chosen at random from the following list (dog, cat, rat, tree, into, nose, without, grandmother, potato, water, telephone, February, desk, jump, light, pencil, funny, triangle, underwater, independent, march, grade, starfish, eleven, sailboat, candle, watermelon) the audio versions of the word were created with a computer generated male voice. The visual stimuli consisted of a randomly chosen word from the same list that was used for the audio presentation. The visual image was font size 20 black times new roman font presented in the center of the screen on a white background for 2 seconds. The third stimuli used in the second screen for all of the decision tasks consisted of a list in descending order of numbers 1-4 in black time’s new roman font size 20.

#### Design

A repeated measure design with the same participant completing all of the conditions will be used. The words that will be used in all of the tasks will be chosen from the same ( dog, cat, rat, tree, into, nose, without, grandmother, potato, water, telephone, February, desk, jump, light, pencil, funny, triangle, underwater, independent, march, grade, starfish, eleven, sailboat, candle, watermelon, pumpkin, lamp, hat, basketball, red, nature, chew, stick, attack, yawn, sleeping, swimming, windfall ) and will be randomized . The independent measure in the first condition was the number of syllables in the words being spoken varying from 1-4 syllables per word. In the first condition the dependent variable was measuring accuracy, response time spent on the second screen shown while picking the number of syllables defined as time (in seconds). The independent measure in the second condition was the number of syllables displayed in the flashed word. The dependent variable was measuring accuracy, and response time spent on the second screen. The third independent variable will be number of syllables in both the audio/visual flashed word. The dependent variable will be accuracy, and response time spent on the second screen. The fourth independent variable is presenting an unpaired or paired form of the visual and auditory word at the same time. The dependent variable in the fourth condition is measuring accuracy and time of both answers.

#### Procedure 

In the first audio procedure each trail begins with a .5 second pause followed by an audio presentation of a word once for 2.5 seconds. After the word is presented there is another an average of 10-20 millisecond pause followed by the screen presenting the list of the number of syllables to choose ranging from 1-4. After a keypress 1-4 indicating the number of syllables in the word a blank screen for 100 milliseconds would be displayed and the second trial would begin. The audio process was completed 27 times. After a 7 minute break the second visual session was began. In the second visual condition each trail begins with a 100 millisecond pause followed by a visual presentation of a word once for 2.5 seconds in the center of the screen. After the word is presented there is pause followed by the screen presenting the list of the number of syllables to choose ranging from 1-4. After a keypress 1-4 indicating the number of syllables in the word a blank screen for 100 milliseconds would be displayed and the second trial would begin. The visual condition was completed 27 times. After a 7 minute break the final audio/visual session was began. In the third audio/visual condition each trail begins with a set of instructions with no time limit. After the participant presses any key the first trial begins with a visual and audio presentation of a word once for 2.5 seconds in the center of the screen with audio of the word playing at the same time. After the word is presented another presents the list of the number of syllables to choose ranging from 1-4. After a keypress 1-4 indicating the number of syllables in the word a blank screen for 100 milliseconds would be displayed and the second trial would begin. The audio/visual condition was completed 27 times. The final condition will be an unpaired or paired matchup of a spoken and written word produced at the same time random pairing. First the screen will display both the written and spoken word for 2.5 seconds. Than the participant will choose the spoken words number of syllables with keys 1-4 directly after a syllable count is chosen another screen will come up and the participant will be prompted to choose the audio word by pressing A = 1 syllable b= 2 syllable c= 3 syllables d = 4 syllables. These trials will happen 27 times.

### Results 

The purpose of this experiment was show that the visual component of language is part of the hierarchical production and comprehension of language both auditory and visually.

### Description of results

Average response times and accuracy percentages were taken from each trial. Results from the first two conditions (audio and visual only) are being used as a baseline comparison to the paired presented audio visual condition. In table 1 response times, and accuracy percentages were calculated for each trial. In the first condition audio only presented the participant scored 92.59% accuracy with the fastest average response time of .7664 seconds. In the second condition visual only the participant scored the highest accuracy of 100% correct responses, but with the slowest response time of .9978 seconds. In the third condition audio and visual was presented together with matching words, the participant scored the lowest accuracy of 88.88% correct and .7737 seconds response time. In the fourth condition where audio and visual were presented together but with random pairing of words the accuracy for visual was 81.48% with an average response time of 3.3702. The accuracy for audio was 88.88% with an average response time of 2.810.

#### Table 1

<table><thead><tr><th style="text-align: left;">Condition</th> <th style="text-align: center;">Accuracy</th> <th style="text-align: right;">Average Response time in seconds</th></tr></thead> <tbody><tr><td style="text-align: left;">Audio only presented</td> <td style="text-align: center;">92.59%</td> <td style="text-align: right;">0.7664</td></tr> <tr><td style="text-align: left;">Visual only presented</td> <td style="text-align: center;">100%</td> <td style="text-align: right;">0.9978</td></tr> <tr><td style="text-align: left;">Audio Visual Paired same time presentation</td> <td style="text-align: center;">88.88%</td> <td style="text-align: right;">0.77374</td></tr></tbody></table>

Furthermore, result times were compared to the other conditions and their difference was calculated (table 2). When comparing audio solo to visual solo, visual solo to paired, audio solo to pair there was no statistical significance. However, when comparing audio solo to mixed audio, visual solo to mix visual, and paired to mix added the difference was considered to be extremely statistically significant.

#### Table 2 : Reaction times compared

<table><thead><tr><th style="text-align: left;">Reaction Time Compared</th> <th style="text-align: center;">Show statistical significance (yes, no)</th> <th style="text-align: right;">Null hypothesis there is no difference between response times.</th></tr></thead> <tbody><tr><td style="text-align: left;">Audio solo vs visual solo</td> <td style="text-align: center;">None</td> <td style="text-align: right;">t(26)=0.07, .94 &gt; .05</td></tr> <tr><td style="text-align: left;">Visual solo vs paired</td> <td style="text-align: center;">None</td> <td style="text-align: right;">t(26)=-0.82, 0.41 &gt; .05</td></tr> <tr><td style="text-align: left;">Audio solo vs paired</td> <td style="text-align: center;">None</td> <td style="text-align: right;">t(26)=-8.23, 1.02&gt; .05</td></tr> <tr><td style="text-align: left;">Audio solo vs. mixed audio</td> <td style="text-align: center;">Extreme statistically significant</td> <td style="text-align: right;">t(26)=8.23, p &lt; .0001</td></tr> <tr><td style="text-align: left;">Visual solo vs mixed visual</td> <td style="text-align: center;">Extreme statistically significant</td> <td style="text-align: right;">t(26)=6.54, p &lt;0.001</td></tr> <tr><td style="text-align: left;">Paired vs. mixed</td> <td style="text-align: center;">Extreme statistically significant</td> <td style="text-align: right;">t(26)=-9.96,p &lt;.0001</td></tr></tbody></table>

#### Interpretation based on key findings.

Currently the hierarchical manor of importance in processing language overlooks the visual aspect in many areas.

The visual aspect of language plays a major role in all aspects of comprehending and giving meaning to language. My hypothesis that language is processed in a hierarchical manor with vision playing a major roll is supported. By studying the reaction times compared to the other conditions it is apparent that in all cases language is processed by imagining at some point with a visual aspect. When comparing the audio only trial to the mixed audio visual trial a blocking of the visual aspect occurs slowing down reaction time and lowering accuracy. When comparing the visual only reaction time to the mixed condition the audio component blocks the ability to comprehend the visually seen word which leads to lower accuracy, and reaction time. Participants were able to realize how many syllables much more efficiently and at nearly the same reaction time when the paired set of stimuli were presents compared to the baselines. It is likely that producing and comprehending language is a constant guess check process rather than previously believed set path to follow.

#### Comparisons/contrasts to other studies

Localization of function does still seem apparent in the processing and comprehending of language, however due to the ability to still respond fairly quickly with over 80% accuracy it is likely that in the background our previously believed major language areas can work as dual purpose production or comprehension areas; or much more likely as a constant circuit team reflecting information through bundled nerve fibers constantly. With that in mind it is likely that Broca’s area, Wernicke’s area, and the arcuate fasciculus are all connected to our visual directly, and separately. It is said that a person with Brocas aphasia cannot produce sound however they can still understand it, this would support the idea that the visual component is the meaning maker. Patients suffering from Wernicke’s aphasia are limited in being able to produce clearly or understand language; often speaking in “word salad” jumbled mixes of syllables therefore Wernicke’s area must serve as the visual “meaning makers” spell checker and syllable organizer by the time the information gets to the Broca’s area like in my mixed word study participants accuracy and comprehension suffers because one area of the visual loop is in this case missing. The multimodal neurons located in the parietal lobe are likely located along our language/visual loop which allow us to do things such as tasting our favorite food when one utters the word, or visually picturing an image of how letters are arranged when spelling a word. Prior studies on perceiving visual stimuli have told us that the best predictor for what subjects were perceiving when presented with an image or asked to imagine a word help support the idea that the occipital lobe is a primary force in all language comprehension due to its help in giving meaning to words. When receiving either visual or audio stimuli of language we form mental images of these stimuli. It is easier to perceive audio presentations of words when in context than when taken out of context; by including the importance of the visual component in all language meaning production, and choosing to block that meaning maker out accuracy and response time was slowed down similar to taking them out of context. Similar to audio it is easier to find letters when they are presented in a word than if they are scattered or in a non-word. The constant hierarchical guess and check language system proposed would support the difficulty in finding those non words. Visual stimuli play a role on our ability to process a sentence because there must be a constant relay of information between the audio/visual loops.

#### Limitations of study

The sample size of participants is a limitation in the study, with a larger sample size one could infer better as a whole how the audio/visual language pathway functions. A voice recognition task to measure response times on auditory production could assist in showing that all the parts are constantly bouncing information between each other. A future direction would be to enlarge the sample size, and complete the trials while utilizing a brain imaging machine such as an fMRI or EEG. Future experiments would benefit in finding more ways to block these processes or slow reaction times too help understand the overall functions of each area.

#### New hypotheses and ideas

Based off of the results of reaction times, and accuracy testing a definite hierarchical language process happens, if you remove one of the components or occupy one of the pieces the reaction time and accuracy is slowed down. Contradictory to the previous belief of the visual component not being used in auditory process of hearing a language the evidence of the mixed vs paired tasks shows otherwise; this is exemplified by the ability to process a word easily and at near same rate/accuracy when it is the same written as it is audio if this were not true than mixed processing should be producing similar numbers to the other conditions. With an fMRI machine or a mind numbing techniques a better understanding to the extent of which our audio/visual language processing loops can compensate for each other. Furthermore, because of the multimodal neurons located in these areas new brain training techniques to aid in therapy of patients suffering from diseases like aphasia.

### Conclusion

Overall the blocking of one of the audio/visual language loop components can greatly reduce accuracy and response time. The visual component of language is the meaning maker, Wernicke’s are is the sorter, and Broca’s area is the motor area the language area can likely become overwhelmed with too many stimuli inhibiting our ability to process stimuli such as language. By studying the multimodal neurons located in these components new language learning techniques can be implemented. By studying the practical uses of other senses to compensate for language areas options like teaching meaning of words through sign language or picture presentation therapy may be viable sources of help.
